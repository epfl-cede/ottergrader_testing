{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b7584d-cb71-45e1-aeba-594afbfaf208",
   "metadata": {},
   "source": [
    "# Grading students' submissions\n",
    "\n",
    "## Overall solution\n",
    "The CLI interface for the autograder works only with docker containers.  \n",
    "If we want to grade without containers, we will need to use other means.\n",
    "\n",
    "In this notebook, we use the `grade_submission` API.  \n",
    "But is is very limited:\n",
    "* It grades single submissions, not batches => we need to automate for batch + use parallel threads for performance?\n",
    "* It returns an object, that we will need to \"parse\" to reconstruct a moodle grading sheet (at least the CLI interface generates a CSV file... maybe it is possible to reuse part of the code?)\n",
    "\n",
    "TODO: for the moment I tested grading on individual .ipynb files, need to test on zip files\n",
    "\n",
    "## How to use it\n",
    "\n",
    "* Go into the dist folder, edit the student version of the assignment and use the public tests\n",
    "* Then try to grade it with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7293fe1e-6a38-44d4-bd29-2e314faa18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from otter.api import grade_submission # grading api\n",
    "import glob # patterns in accessing file paths\n",
    "import pandas as pd # pandas for saving as csv\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import csv # csv quoting options\n",
    "from datetime import datetime # date formatting\n",
    "from IPython.display import display # display for debug\n",
    "import itertools as it # iteration tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd7dbe-6254-4a4c-ad66-5eb6ef7b5496",
   "metadata": {},
   "source": [
    "## Grading parameters\n",
    "\n",
    "Assignment parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a2a7e-63ff-4901-ae8e-8fb2822dff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the assignment **file**\n",
    "assignmentname = \"assignment\"\n",
    "# Folder in which the assignment and grader has been generated\n",
    "distributionfolder = \"dist\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3c52b-939d-4cc0-bbd9-126382f280e9",
   "metadata": {},
   "source": [
    "Grader parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d12f45-6dc3-4d65-83eb-a372d6cb4c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder in which to put the output of the grading\n",
    "graderoutputfolder = \"gradebook\"\n",
    "# Name of the CSV file in which to store the details of the grading\n",
    "graderdetailsfilename = \"graderdetails\"\n",
    "# Name of the CSV file with the overall grade (later used to fill out the moodle file)\n",
    "graderresultfilename = \"graderresult\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d03d0-6e12-49c3-a250-86b1dbdcf0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the messages from the test cases as feedback for the student\n",
    "includefeedbackmessages = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f4305-9897-442e-bffd-8bc5a3d339f4",
   "metadata": {},
   "source": [
    "Submission parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917a1d5-d58a-4cb4-9537-74820b0bc193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder in which to find students' submission\n",
    "submissionfolder = \"moodlesubmissions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e88648-42a9-4572-88bf-5ece996b1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where to find the moodle grading sheets\n",
    "moodlegradingsheetfolder = \"gradebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4aa7bd-1203-48b4-b7b7-d3e48de565ea",
   "metadata": {},
   "source": [
    "## Grading and generation of the output\n",
    "\n",
    "First retrieving the grader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057db5b0-aed6-44a2-8199-8bddb34ff1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graderzip = glob.glob(distributionfolder+\"/autograder/\"+assignmentname+\"-autograder_*.zip\")[0]\n",
    "graderzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7651bf6e-95cf-473d-b095-d9c438e148ee",
   "metadata": {},
   "source": [
    "Then retrieving students' submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004dc5b8-7b5b-4a2a-9ac1-5378fb1fa60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissionlist = glob.glob(submissionfolder+\"/*\")\n",
    "submissionlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b12e2-e05e-4377-97f6-4c24fdafe50c",
   "metadata": {},
   "source": [
    "Iterating over submissions, calling the grader and storing the results into CSV files\n",
    "\n",
    "**TODO**:\n",
    "* Check if the CSV with the details is still readable with the line returns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae66c74-c607-4645-8fe3-53cc57dae08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: generate 2 CSV files for each student: 1 with the overall grade and 1 with the detailed grading\n",
    "for submission in submissionlist:\n",
    "    \n",
    "    # Call the autograder and get the result\n",
    "    gradingoutput = grade_submission(submission+\"/\"+assignmentname+\".ipynb\", graderzip, quiet=True)\n",
    "    \n",
    "    # Gather the details of the grading\n",
    "    detailedresults = [[\"Exercise\", \"Grade\", \"Possible\", \"Feedback\"]]\n",
    "    for exercisename, exercise in gradingoutput.results.items():\n",
    "        feedbackmessage = \"\"\n",
    "        \n",
    "        # Iterate over the test results to collect the feedback messages if any\n",
    "        if includefeedbackmessages:\n",
    "            for test_case_result in exercise.test_case_results:\n",
    "                if (test_case_result.passed and (test_case_result.test_case.success_message is not None)):\n",
    "                    feedbackmessage+= test_case_result.test_case.name+\": \"+test_case_result.test_case.success_message+\"\\n\"\n",
    "                elif ((not test_case_result.passed) and (test_case_result.test_case.failure_message is not None)):\n",
    "                    feedbackmessage+= test_case_result.test_case.name+\": \"+test_case_result.test_case.failure_message+\"\\n\"\n",
    "        \n",
    "        detailedresults.append([exercise.name, exercise.score, exercise.possible, feedbackmessage])\n",
    "\n",
    "    # Save the details of the grading\n",
    "    graderdetailsfile = submission+\"/\"+graderoutputfolder+\"/\"+graderdetailsfilename+\".csv\"\n",
    "    pd.DataFrame(detailedresults).to_csv(graderdetailsfile, header=False, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "    \n",
    "    # Gather the total grade for the submission and save it\n",
    "    graderresultfile = submission+\"/\"+graderoutputfolder+\"/\"+graderresultfilename+\".csv\"\n",
    "    graderresultdf = pd.DataFrame([[submission, gradingoutput.total, gradingoutput.possible]], \n",
    "                                  columns=[\"Submission Name\", \"Grade\", \"Possible\"])\n",
    "    graderresultdf.to_csv(graderresultfile, index=False, quoting=csv.QUOTE_NONNUMERIC)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7069816e-f38f-4add-b3b3-1456c1a6eb26",
   "metadata": {},
   "source": [
    "## Filling out the CSV files that are specific to moodle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fdec2-187e-4709-ae8c-fa222caee651",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "* generate the feedback: detail of points + messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8776ef6-bc34-4e17-837f-659e4cb00b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include details into overall moodle feedback\n",
    "includedetailsgrades = True\n",
    "includedetailsmsgs = True # NB: messages will be included only if grades are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5892db-903c-4056-a591-1e487d4eb396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: fill out the individual CSV from moodle for each student\n",
    "for submission in submissionlist:\n",
    "\n",
    "    # Read the moodle file\n",
    "    moodlegradebookfile = glob.glob(submission+\"/\"+moodlegradingsheetfolder+\"/*_grading.csv\")[0]\n",
    "    moodledf = pd.read_csv(moodlegradebookfile)\n",
    "    \n",
    "    # Read the grader file\n",
    "    graderresultfile = glob.glob(submission+\"/\"+graderoutputfolder+\"/\"+graderresultfilename+\".csv\")[0]\n",
    "    graderresultdf = pd.read_csv(graderresultfile)\n",
    "    \n",
    "    if includedetailsgrades:\n",
    "        # Read the grader details file\n",
    "        graderdetailsfile = glob.glob(submission+\"/\"+graderoutputfolder+\"/\"+graderdetailsfilename+\".csv\")[0]\n",
    "        graderdetailstdf = pd.read_csv(graderdetailsfile)\n",
    "        \n",
    "        # Formatters to get a pretty rendering of the info\n",
    "        exerciseformatter = lambda x: 'Question %s:' % x\n",
    "        gradeformatter = lambda x: '%s /' % x\n",
    "        msgformatter = lambda x: '=> Messages: %s' % x.replace(\"\\n\", \"\")\n",
    "        \n",
    "        # Transform the details into string\n",
    "        details = graderdetailstdf.to_string(index=False, header=False, \n",
    "                                         columns=[\"Exercise\", \"Grade\", \"Possible\"] + ([\"Feedback\"] if includedetailsmsgs else []),\n",
    "                                         formatters={'Exercise': exerciseformatter, 'Grade': gradeformatter, 'Feedback': msgformatter})\n",
    "        \n",
    "        # Add the details to the moodle info\n",
    "        moodledf.loc[0, \"Feedback comments\"] = details\n",
    "\n",
    "        \n",
    "    # Modify the moodle info\n",
    "    moodledf.loc[0, \"Grade\"] = graderresultdf.loc[0, \"Grade\"]\n",
    "    moodledf.loc[0, \"Maximum Grade\"] = graderresultdf.loc[0, \"Possible\"] # Actually this line is useless, it is not used by moodle\n",
    "    moodledf.loc[0, \"Last modified (grade)\"] = datetime.today().strftime('%A, %d %B %Y %H:%M') # This line is important for the file to be read by moodle!!!\n",
    "    \n",
    "    \n",
    "    # Write the moodle file\n",
    "    moodledf.to_csv(moodlegradebookfile, index=False, quoting=csv.QUOTE_NONNUMERIC) # The quotes are important for the file to be read by moodle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3539d-947f-4c7a-af94-cdbeecc2ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: concatenate all moodle CSV files into 1 global gradebook"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd2f90f0-9637-4cd1-a098-ebacae4bda76",
   "metadata": {},
   "source": [
    "results = []\n",
    "for submission in submissionlist:\n",
    "    gradingoutput = grade_submission(submission, graderzip, quiet=True)\n",
    "    feedbackstr = \"Detailed feedback:\\n\" \n",
    "    for exercisename, exercise in gradingoutput.results.items():\n",
    "        feedbackstr += \"Exercise \"+ exercise.name + \": grade = \" + str(exercise.grade) + \" / \" + str(exercise.possible) + \"\\n\"\n",
    "    results.append([submission, gradingoutput.total, gradingoutput.possible, feedbackstr])\n",
    "\n",
    "resultsdf = pd.DataFrame(data=results, columns=[\"studentname\", \"grade\", \"maxgrade\", \"feedbackcomments\"])\n",
    "pretty_print(resultsdf)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49009ed5-73d0-4591-ba6d-aabd226c7847",
   "metadata": {},
   "source": [
    "grade_submission('submissions/student1234/assignment1234.ipynb', graderzip, quiet=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5f82cb2-f6a2-45fb-b2f6-199237d60926",
   "metadata": {},
   "source": [
    "print(resultsobject.total, \"/\", resultsobject.possible)\n",
    "for exercisename, exercise in resultsobject.results.items():\n",
    "    print(exercise.name)\n",
    "    print(exercise.grade)\n",
    "    print(exercise.possible)\n",
    "    print(exercise.passed_all)\n",
    "    print(exercise.passed_all_public)\n",
    "    for test_case_result in exercise.test_case_results:\n",
    "        print(\"\\t\", test_case_result.test_case.name)\n",
    "        print(\"\\t\", test_case_result.test_case.points)\n",
    "        print(\"\\t\", test_case_result.passed)\n",
    "        print(\"\\t\", test_case_result.test_case.success_message if test_case_result.passed else test_case_result.test_case.failure_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
